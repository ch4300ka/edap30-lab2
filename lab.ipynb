{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971d9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d909ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"sk-RpSfTpeq-El4GvTwz_QHEQ\"\n",
    "API_URL = \"http://api-uat-be.waraml.org/v1\"\n",
    "WARA_API_KEY = \"waraml-sLoYpybG81wZCx29sAGQWJhA\"\n",
    "\n",
    "INPUT_PROMPT = \"Generate a Python code snippet that causes an error. Return only the code and the resulting error message.\"\n",
    "OUTPUT_PROMPT = \"The following Python code causes errors. Please explain why it happens and how to fix it.\"\n",
    "\n",
    "DATASET_JSON_PATH = Path(\"data/dataset.json\")\n",
    "TRAIN_DATASET_PATH = Path(\"data/train_dataset.json\")\n",
    "TRAIN_DATASET_ZIP_PATH = Path(\"data/train_dataset.zip\")\n",
    "EVAL_DATASET_PATH = Path(\"data/eval_dataset.json\")\n",
    "EVAL_DATASET_ZIP_PATH = Path(\"data/eval_dataset.zip\")\n",
    "EVAL_RESULTS_PATH = Path(\"data/eval_results\")\n",
    "METADATA_JSON_PATH = Path(\"data/metadata.json\")\n",
    "\n",
    "NUM_SAMPLES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18971c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"azure-gpt-4o\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 2048,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"{API_URL}/chat/completions\", headers=headers, json=data)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8d2b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(TRAIN_DATASET_ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(TRAIN_DATASET_PATH, arcname=TRAIN_DATASET_PATH.name)\n",
    "    \n",
    "with zipfile.ZipFile(EVAL_DATASET_ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(EVAL_DATASET_PATH, arcname=EVAL_DATASET_PATH.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3dddf997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "    \n",
    "X_API_KEY = \"waraml-sLoYpybG81wZCx29sAGQWJhA\"\n",
    "BASE_JOB_URL = \"https://llmhubapi.waraml.org/jobs\"\n",
    "\n",
    "params = {\n",
    "    \"job_name\": \"PythonErrorModel\",\n",
    "    \"base_model\": \"gpt2\",\n",
    "    \"lora_r\": \"8\",\n",
    "    \"num_epochs\": \"5\",\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": \"0.0002\",\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"X-API-Key\": X_API_KEY,\n",
    "}\n",
    "\n",
    "def save_checkpoint(job_id):\n",
    "\n",
    "    resp = requests.get(\n",
    "        f\"{BASE_JOB_URL}/{job_id}/download\", headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.content\n",
    "\n",
    "    ckpt_dir = f\"{job_id}_lora_checkpoint\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    if data[:2] == b\"PK\":\n",
    "        zip_path = os.path.join(ckpt_dir, \"adapter.zip\")\n",
    "        with open(zip_path, \"wb\") as f:\n",
    "            f.write(data)\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            z.extractall(ckpt_dir)\n",
    "    else:\n",
    "        # save the raw adapter\n",
    "        adapter_file = os.path.join(ckpt_dir, \"adapter_model.safetensors\")\n",
    "        with open(adapter_file, \"wb\") as f:\n",
    "            f.write(data)\n",
    "        # write a full adapter_config.json so PEFT can load it\n",
    "        config = {\n",
    "            \"peft_type\":        \"LORA\",\n",
    "            \"task_type\":        \"CAUSAL_LM\",\n",
    "            \"inference_mode\":   False,\n",
    "            \"r\":                8,\n",
    "            \"lora_alpha\":       32,\n",
    "            \"lora_dropout\":     0.05,\n",
    "            # GPT-2 LoRA usually targets the attention proj & attn matrices:\n",
    "            \"target_modules\":   [\"c_attn\", \"c_proj\"],\n",
    "            \"bias\":             \"none\"\n",
    "        }\n",
    "        with open(os.path.join(ckpt_dir, \"adapter_config.json\"), \"w\") as f:\n",
    "            json.dump(config, f)\n",
    "\n",
    "    print(f\"Checkpoint prepared in {ckpt_dir}/\")\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "def start_finetune( dataset_zip_path):\n",
    "\n",
    "    print(f\"\\nüîÅ Starting job {params['job_name']}\")\n",
    "\n",
    "    # Prepare file upload\n",
    "    files = {\n",
    "        \"dataset_file\": open(dataset_zip_path, \"rb\")\n",
    "    }\n",
    "\n",
    "    # Submit training job\n",
    "    response = requests.post(BASE_JOB_URL, headers=headers, params=params, files=files)\n",
    "\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "        message = response.json()\n",
    "        job_id = message.get(\"id\")\n",
    "        print(f\"‚úÖ Job {job_id} submitted.\")\n",
    "    except requests.HTTPError:\n",
    "        print(\"‚ùå Job submission failed:\", response.status_code)\n",
    "        print(response.text)\n",
    "        return\n",
    "\n",
    "    current_epoch = 1\n",
    "    \n",
    "    # Poll job status\n",
    "    while True:\n",
    "        status_resp = requests.get(f\"{BASE_JOB_URL}/{job_id}\", headers=headers)\n",
    "        status_data = status_resp.json()\n",
    "        job_status = status_data.get(\"status\")\n",
    "        metrics = status_data.get(\"metrics\")\n",
    "\n",
    "        if metrics:\n",
    "            print(f\"Metrics:\")\n",
    "            print(json.dumps(metrics, indent=2))\n",
    "            current_epoch = metrics.get(\"current_epoch\")\n",
    "            \n",
    "        if job_status == \"completed\":\n",
    "            print(f\"‚úÖ Job {job_id} completed\")\n",
    "            break\n",
    "        elif job_status == \"failed\":\n",
    "            print(f\"‚ùå Training failed at epoch {current_epoch} (Job {job_id})\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"‚è≥ Waiting for epoch {current_epoch} to complete...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "    save_checkpoint(job_id)\n",
    "\n",
    "    return job_id\n",
    "   \n",
    "def evaluate_model(job_id):\n",
    "    with open(EVAL_DATASET_ZIP_PATH, \"rb\") as f:\n",
    "        files = {\n",
    "            \"test_dataset\": f\n",
    "        }\n",
    "\n",
    "        response = requests.post(\n",
    "            f\"{BASE_JOB_URL}/{job_id}/evaluate\",\n",
    "            headers=headers,\n",
    "            files=files\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "            message = response.json()\n",
    "            print(\"Evaluation status:\")\n",
    "            print(json.dumps(message, indent=2))\n",
    "        except requests.HTTPError:\n",
    "            print(\"Failed:\", response.status_code)\n",
    "            print(json.dumps(response.text, indent=2))\n",
    "            \n",
    "def poll_eval_status(job_id):\n",
    "    while True:\n",
    "        response = requests.get(f\"{BASE_JOB_URL}/{job_id}\", headers=headers)\n",
    "        \n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "            status_data = response.json()\n",
    "\n",
    "            status = status_data.get(\"status\")\n",
    "            print(f\"Evaluation status: {status}\")\n",
    "\n",
    "            metrics = status_data.get(\"metrics\")\n",
    "            if metrics:\n",
    "                print(f\"Metrics:\")\n",
    "                print(json.dumps(metrics, indent=2))\n",
    "\n",
    "            if status == \"completed\":\n",
    "                print(\"Evaluation completed!\")\n",
    "                return status_data\n",
    "            elif status == \"failed\":\n",
    "                print(\"Evaluation failed.\")\n",
    "                return status_data\n",
    "\n",
    "        except requests.HTTPError as e:\n",
    "            print(\"Failed to get evaluation status:\", e)\n",
    "            print(response.text)\n",
    "            break\n",
    "                \n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b36096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Starting job PythonErrorModel\n",
      "‚úÖ Job 2270 submitted.\n",
      "Metrics:\n",
      "{\n",
      "  \"cuda_available\": true,\n",
      "  \"gpu_name\": \"NVIDIA GeForce RTX 3090\"\n",
      "}\n",
      "‚è≥ Waiting for epoch None to complete...\n",
      "Metrics:\n",
      "{\n",
      "  \"cuda_available\": true,\n",
      "  \"gpu_name\": \"NVIDIA GeForce RTX 3090\",\n",
      "  \"current_epoch\": 4.44,\n",
      "  \"current_loss\": 1.2079,\n",
      "  \"last_update\": 1748185527.436311,\n",
      "  \"training_step\": 0\n",
      "}\n",
      "‚è≥ Waiting for epoch 4.44 to complete...\n",
      "Metrics:\n",
      "{\n",
      "  \"cuda_available\": true,\n",
      "  \"gpu_name\": \"NVIDIA GeForce RTX 3090\",\n",
      "  \"current_epoch\": 5.0,\n",
      "  \"current_loss\": 0,\n",
      "  \"last_update\": 1748185535.1581683,\n",
      "  \"training_step\": 0,\n",
      "  \"train_runtime\": 65.6369,\n",
      "  \"train_samples_per_second\": 16.454,\n",
      "  \"train_loss\": 1.6621660903648094,\n",
      "  \"eval_loss\": 1.1080495119094849,\n",
      "  \"perplexity\": 3.0284456827363786,\n",
      "  \"training_time\": \"0:01:07.983891\",\n",
      "  \"base_model\": \"gpt2\",\n",
      "  \"device\": \"cuda\",\n",
      "  \"num_examples\": 216,\n",
      "  \"num_epochs\": 5,\n",
      "  \"final_loss\": 1.6621660903648094,\n",
      "  \"lora_rank\": 8,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"status\": \"completed\"\n",
      "}\n",
      "‚úÖ Job 2270 completed\n",
      "Checkpoint prepared in 2270_lora_checkpoint/\n"
     ]
    }
   ],
   "source": [
    "job_id = start_finetune(TRAIN_DATASET_ZIP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65e82565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation status:\n",
      "{\n",
      "  \"message\": \"Evaluation started for job 2270\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f6ac1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation status: completed\n",
      "Metrics:\n",
      "{\n",
      "  \"cuda_available\": true,\n",
      "  \"gpu_name\": \"NVIDIA GeForce RTX 3090\",\n",
      "  \"current_epoch\": 5.0,\n",
      "  \"current_loss\": 0,\n",
      "  \"last_update\": 1748185535.1581683,\n",
      "  \"training_step\": 0,\n",
      "  \"train_runtime\": 65.6369,\n",
      "  \"train_samples_per_second\": 16.454,\n",
      "  \"train_loss\": 1.6621660903648094,\n",
      "  \"eval_loss\": 1.1080495119094849,\n",
      "  \"perplexity\": 3.0284456827363786,\n",
      "  \"training_time\": \"0:01:07.983891\",\n",
      "  \"base_model\": \"gpt2\",\n",
      "  \"device\": \"cuda\",\n",
      "  \"num_examples\": 216,\n",
      "  \"num_epochs\": 5,\n",
      "  \"final_loss\": 1.6621660903648094,\n",
      "  \"lora_rank\": 8,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"status\": \"completed\",\n",
      "  \"evaluation_cuda_available\": true,\n",
      "  \"evaluation_gpu_name\": \"NVIDIA GeForce RTX 3090\",\n",
      "  \"evaluation_metrics\": {\n",
      "    \"num_samples\": 50,\n",
      "    \"avg_completion_length\": 243.2,\n",
      "    \"min_completion_length\": 125,\n",
      "    \"max_completion_length\": 321,\n",
      "    \"avg_tokens\": 98.36,\n",
      "    \"min_tokens\": 54,\n",
      "    \"max_tokens\": 100,\n",
      "    \"avg_bleu\": 0.005669194234508521,\n",
      "    \"min_bleu\": 7.834790622988904e-05,\n",
      "    \"max_bleu\": 0.05633970973294052,\n",
      "    \"exact_match_rate\": 0.0,\n",
      "    \"rouge1\": 0.15598347142554184,\n",
      "    \"rouge2\": 0.056698958017921186,\n",
      "    \"rougeL\": 0.11621254075106421,\n",
      "    \"evaluation_path\": \"/app/app/uploads/evaluations/job_2270\"\n",
      "  },\n",
      "  \"evaluation_path\": \"/app/app/uploads/evaluations/job_2270\"\n",
      "}\n",
      "Evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "status_data = poll_eval_status(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d15457ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded evaluation results to: data\\eval_results.zip\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(\"data/eval_results.zip\")\n",
    "eval_extract_to = Path(\"data/eval_results/\")\n",
    "\n",
    "response = requests.get(\n",
    "    f\"{BASE_JOB_URL}/{job_id}/evaluation/download\",\n",
    "    headers=headers,\n",
    "    stream=True)\n",
    "response.raise_for_status()\n",
    "\n",
    "with open(output_path, \"wb\") as f:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        if chunk:\n",
    "            f.write(chunk)\n",
    "\n",
    "print(f\"Downloaded evaluation results to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d8c35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "eval_extract_to.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(output_path, 'r') as zf:\n",
    "    zf.extractall(eval_extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1edfc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalls\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\peft\\tuners\\lora\\layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Output:\n",
      " The following Python code causes errors. Please explain why it happens and how to fix it.```python\n",
      "# Code snippet\n",
      "result = 1 / 0\n",
      "\n",
      "# Error message\n",
      "ZeroDivisionError: division by zero\n",
      "```\n",
      "\n",
      "The code below is a Python function that returns a number divisible by zero. The code below returns the number divisible by zero:\n",
      "```python\n",
      "# Code snippet\n",
      "result = 1 / 0\n",
      "\n",
      "# Error message\n",
      "ZeroDivision\n",
      "\n",
      "LoRA Fine-Tuned Model Output:\n",
      " The following Python code causes errors. Please explain why it happens and how to fix it.```python\n",
      "# Code snippet\n",
      "result = 1 / 0\n",
      "\n",
      "# Error message\n",
      "ZeroDivisionError: division by zero\n",
      "```\n",
      "\n",
      "The code below is a Python function that returns a number divisible by zero. The code below returns the number divisible by zero:\n",
      "```python\n",
      "# Code snippet\n",
      "result = 1 / 0\n",
      "\n",
      "# Error message\n",
      "ZeroDivision\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Fix missing pad token for GPT-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load LoRA-adapted model\n",
    "model = PeftModel.from_pretrained(base_model, f\"{job_id}_lora_checkpoint/\")\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Input prompt\n",
    "input_text = \"\"\"The following Python code causes errors. Please explain why it happens and how to fix it.\n",
    "python\\n# Code snippet\\nprint(\\\"The result is \\\" + 5)\\n\\n# Error message\\nTypeError: can only concatenate str (not \\\"int\\\") to str\\n```\",\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Generate outputs\n",
    "tuned_outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=100)\n",
    "outputs = base_model.generate(input_ids, attention_mask=attention_mask, max_length=100)\n",
    "\n",
    "# Decode\n",
    "tuned_result = tokenizer.decode(tuned_outputs[0], skip_special_tokens=True)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Compare\n",
    "print(\"Base Model Output:\\n\", result)\n",
    "print(\"\\nLoRA Fine-Tuned Model Output:\\n\", tuned_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdb4ba3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain and fix this Python error: ```python\n",
      "# Code snippet\n",
      "result = 1 / 0\n",
      "\n",
      "# Error message\n",
      "ZeroDivisionError: division by zero\n",
      "```\n",
      "Base model response: Explain and fix this Python error: ```python\n",
      "# Code snippet\n",
      "result = 1 / 0\n",
      "\n",
      "# Error message\n",
      "ZeroDivisionError: division by zero\n",
      "```\n",
      "\n",
      "The Python error message is a Python exception that occurs when a division is attempted. The Python exception is thrown when a division is attempted, and the result of the division is undefined.\n",
      "\n",
      "The Python error message is thrown when a division is attempted, and the result of the division is undefined.\n",
      "\n",
      "Tuned model response: Explain and fix this Python error: ```python\n",
      "# Code snippet\n",
      "result = 1 / 0\n",
      "\n",
      "# Error message\n",
      "ZeroDivisionError: division by zero\n",
      "```\n",
      "\n",
      "The Python error message is a Python exception that occurs when a division is attempted. The Python exception is thrown when a division is attempted, and the result of the division is undefined.\n",
      "\n",
      "The Python error message is thrown when a division is attempted, and the result of the division is undefined.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prompt: {input_text}\\nBase model response: {result}\\nTuned model response: {tuned_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1fb803e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight 14659.9580078125\n",
      "transformer.wpe.weight -533.7647094726562\n",
      "transformer.h.0.ln_1.weight 138.5156707763672\n",
      "transformer.h.0.ln_1.bias -5.0637712478637695\n",
      "transformer.h.0.attn.c_attn.weight 94.44642639160156\n",
      "transformer.h.0.attn.c_attn.bias -1.6296703815460205\n",
      "transformer.h.0.attn.c_proj.weight -95.18525695800781\n",
      "transformer.h.0.attn.c_proj.bias -5.307050704956055\n",
      "transformer.h.0.ln_2.weight 666.4932250976562\n",
      "transformer.h.0.ln_2.bias 7.068312168121338\n",
      "transformer.h.0.mlp.c_fc.weight -1766.00537109375\n",
      "transformer.h.0.mlp.c_fc.bias -286.1942138671875\n",
      "transformer.h.0.mlp.c_proj.weight 18.895719528198242\n",
      "transformer.h.0.mlp.c_proj.bias -0.324904203414917\n",
      "transformer.h.1.ln_1.weight 171.1417999267578\n",
      "transformer.h.1.ln_1.bias -3.858037233352661\n",
      "transformer.h.1.attn.c_attn.weight 49.700870513916016\n",
      "transformer.h.1.attn.c_attn.bias 1.8437765836715698\n",
      "transformer.h.1.attn.c_proj.weight -48.81307601928711\n",
      "transformer.h.1.attn.c_proj.bias -0.8238444328308105\n",
      "transformer.h.1.ln_2.weight 186.3890380859375\n",
      "transformer.h.1.ln_2.bias -3.1288247108459473\n",
      "transformer.h.1.mlp.c_fc.weight 1514.740234375\n",
      "transformer.h.1.mlp.c_fc.bias -221.79351806640625\n",
      "transformer.h.1.mlp.c_proj.weight 231.0700225830078\n",
      "transformer.h.1.mlp.c_proj.bias 0.19243967533111572\n",
      "transformer.h.2.ln_1.weight 184.91139221191406\n",
      "transformer.h.2.ln_1.bias -0.27653980255126953\n",
      "transformer.h.2.attn.c_attn.weight 145.37283325195312\n",
      "transformer.h.2.attn.c_attn.bias -8.931851387023926\n",
      "transformer.h.2.attn.c_proj.weight -19.461074829101562\n",
      "transformer.h.2.attn.c_proj.bias 2.592454195022583\n",
      "transformer.h.2.ln_2.weight 224.70718383789062\n",
      "transformer.h.2.ln_2.bias 4.875159740447998\n",
      "transformer.h.2.mlp.c_fc.weight -11941.009765625\n",
      "transformer.h.2.mlp.c_fc.bias -285.1502685546875\n",
      "transformer.h.2.mlp.c_proj.weight 465.13201904296875\n",
      "transformer.h.2.mlp.c_proj.bias 2.1652653217315674\n",
      "transformer.h.3.ln_1.weight 231.24313354492188\n",
      "transformer.h.3.ln_1.bias 4.183685302734375\n",
      "transformer.h.3.attn.c_attn.weight -43.32380676269531\n",
      "transformer.h.3.attn.c_attn.bias -2.0472640991210938\n",
      "transformer.h.3.attn.c_proj.weight 19.901512145996094\n",
      "transformer.h.3.attn.c_proj.bias -1.1990697383880615\n",
      "transformer.h.3.ln_2.weight 235.39796447753906\n",
      "transformer.h.3.ln_2.bias 7.653310775756836\n",
      "transformer.h.3.mlp.c_fc.weight -14037.8701171875\n",
      "transformer.h.3.mlp.c_fc.bias -284.257568359375\n",
      "transformer.h.3.mlp.c_proj.weight 416.3130187988281\n",
      "transformer.h.3.mlp.c_proj.bias 1.6143553256988525\n",
      "transformer.h.4.ln_1.weight 245.25732421875\n",
      "transformer.h.4.ln_1.bias 6.079708576202393\n",
      "transformer.h.4.attn.c_attn.weight 269.065673828125\n",
      "transformer.h.4.attn.c_attn.bias 11.983919143676758\n",
      "transformer.h.4.attn.c_proj.weight -6.721630573272705\n",
      "transformer.h.4.attn.c_proj.bias -0.6922200918197632\n",
      "transformer.h.4.ln_2.weight 209.3428497314453\n",
      "transformer.h.4.ln_2.bias 0.7370175123214722\n",
      "transformer.h.4.mlp.c_fc.weight -7699.9501953125\n",
      "transformer.h.4.mlp.c_fc.bias -264.57952880859375\n",
      "transformer.h.4.mlp.c_proj.weight 424.6252746582031\n",
      "transformer.h.4.mlp.c_proj.bias 1.2333612442016602\n",
      "transformer.h.5.ln_1.weight 286.5557556152344\n",
      "transformer.h.5.ln_1.bias 9.1210355758667\n",
      "transformer.h.5.attn.c_attn.weight -191.9371795654297\n",
      "transformer.h.5.attn.c_attn.bias 2.408836841583252\n",
      "transformer.h.5.attn.c_proj.weight 6.481473922729492\n",
      "transformer.h.5.attn.c_proj.bias -0.8973903656005859\n",
      "transformer.h.5.ln_2.weight 214.2735595703125\n",
      "transformer.h.5.ln_2.bias 6.260713577270508\n",
      "transformer.h.5.mlp.c_fc.weight -9894.5986328125\n",
      "transformer.h.5.mlp.c_fc.bias -261.1981506347656\n",
      "transformer.h.5.mlp.c_proj.weight 273.6515197753906\n",
      "transformer.h.5.mlp.c_proj.bias 0.7321228981018066\n",
      "transformer.h.6.ln_1.weight 265.4195251464844\n",
      "transformer.h.6.ln_1.bias 9.079648971557617\n",
      "transformer.h.6.attn.c_attn.weight 197.52667236328125\n",
      "transformer.h.6.attn.c_attn.bias 4.196219444274902\n",
      "transformer.h.6.attn.c_proj.weight 21.80832290649414\n",
      "transformer.h.6.attn.c_proj.bias -0.3351450562477112\n",
      "transformer.h.6.ln_2.weight 199.27215576171875\n",
      "transformer.h.6.ln_2.bias 3.326369285583496\n",
      "transformer.h.6.mlp.c_fc.weight -6651.21875\n",
      "transformer.h.6.mlp.c_fc.bias -263.27734375\n",
      "transformer.h.6.mlp.c_proj.weight 225.26490783691406\n",
      "transformer.h.6.mlp.c_proj.bias 1.2002882957458496\n",
      "transformer.h.7.ln_1.weight 273.846923828125\n",
      "transformer.h.7.ln_1.bias 11.015841484069824\n",
      "transformer.h.7.attn.c_attn.weight -161.06787109375\n",
      "transformer.h.7.attn.c_attn.bias -9.867831230163574\n",
      "transformer.h.7.attn.c_proj.weight 14.094234466552734\n",
      "transformer.h.7.attn.c_proj.bias -0.10262787342071533\n",
      "transformer.h.7.ln_2.weight 196.61878967285156\n",
      "transformer.h.7.ln_2.bias 7.053074836730957\n",
      "transformer.h.7.mlp.c_fc.weight -8311.771484375\n",
      "transformer.h.7.mlp.c_fc.bias -271.78607177734375\n",
      "transformer.h.7.mlp.c_proj.weight 204.05416870117188\n",
      "transformer.h.7.mlp.c_proj.bias 0.9158498048782349\n",
      "transformer.h.8.ln_1.weight 257.4535217285156\n",
      "transformer.h.8.ln_1.bias 10.177390098571777\n",
      "transformer.h.8.attn.c_attn.weight -301.292724609375\n",
      "transformer.h.8.attn.c_attn.bias -12.740524291992188\n",
      "transformer.h.8.attn.c_proj.weight 4.79150390625\n",
      "transformer.h.8.attn.c_proj.bias 0.8338897824287415\n",
      "transformer.h.8.ln_2.weight 197.1121826171875\n",
      "transformer.h.8.ln_2.bias 0.29521554708480835\n",
      "transformer.h.8.mlp.c_fc.weight -4875.56640625\n",
      "transformer.h.8.mlp.c_fc.bias -261.2983093261719\n",
      "transformer.h.8.mlp.c_proj.weight 110.7016372680664\n",
      "transformer.h.8.mlp.c_proj.bias 0.8878742456436157\n",
      "transformer.h.9.ln_1.weight 274.6030578613281\n",
      "transformer.h.9.ln_1.bias 12.21374797821045\n",
      "transformer.h.9.attn.c_attn.weight -123.6069564819336\n",
      "transformer.h.9.attn.c_attn.bias 0.6577854156494141\n",
      "transformer.h.9.attn.c_proj.weight -16.446731567382812\n",
      "transformer.h.9.attn.c_proj.bias 1.6389665603637695\n",
      "transformer.h.9.ln_2.weight 203.5008544921875\n",
      "transformer.h.9.ln_2.bias 4.915767192840576\n",
      "transformer.h.9.mlp.c_fc.weight -6466.55078125\n",
      "transformer.h.9.mlp.c_fc.bias -257.0263977050781\n",
      "transformer.h.9.mlp.c_proj.weight 82.08599853515625\n",
      "transformer.h.9.mlp.c_proj.bias 0.548147439956665\n",
      "transformer.h.10.ln_1.weight 290.4635925292969\n",
      "transformer.h.10.ln_1.bias 14.294629096984863\n",
      "transformer.h.10.attn.c_attn.weight 163.68099975585938\n",
      "transformer.h.10.attn.c_attn.bias 3.7113070487976074\n",
      "transformer.h.10.attn.c_proj.weight -0.5189604759216309\n",
      "transformer.h.10.attn.c_proj.bias 1.5545157194137573\n",
      "transformer.h.10.ln_2.weight 222.48509216308594\n",
      "transformer.h.10.ln_2.bias 16.250146865844727\n",
      "transformer.h.10.mlp.c_fc.weight -7568.73095703125\n",
      "transformer.h.10.mlp.c_fc.bias -235.08474731445312\n",
      "transformer.h.10.mlp.c_proj.weight 14.404132843017578\n",
      "transformer.h.10.mlp.c_proj.bias 1.2742655277252197\n",
      "transformer.h.11.ln_1.weight 367.63629150390625\n",
      "transformer.h.11.ln_1.bias 17.88226318359375\n",
      "transformer.h.11.attn.c_attn.weight 95.57463073730469\n",
      "transformer.h.11.attn.c_attn.bias 1.686198353767395\n",
      "transformer.h.11.attn.c_proj.weight -31.719636917114258\n",
      "transformer.h.11.attn.c_proj.bias -16.516115188598633\n",
      "transformer.h.11.ln_2.weight 387.15350341796875\n",
      "transformer.h.11.ln_2.bias 7.06006383895874\n",
      "transformer.h.11.mlp.c_fc.weight -4356.24267578125\n",
      "transformer.h.11.mlp.c_fc.bias -196.95896911621094\n",
      "transformer.h.11.mlp.c_proj.weight -1027.0499267578125\n",
      "transformer.h.11.mlp.c_proj.bias 0.7462091445922852\n",
      "transformer.ln_f.weight 1157.9970703125\n",
      "transformer.ln_f.bias -2.4103431701660156\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "for name, param in base_model.named_parameters():\n",
    "    print(name, torch.sum(param).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
